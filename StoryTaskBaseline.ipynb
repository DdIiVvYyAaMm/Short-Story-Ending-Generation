{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level Language Models\n",
    "\n",
    "### [Derived from a blog post by Yoav Goldberg](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsmoothed Maximum Likelihood Character Level Language Model \n",
    "\n",
    "The name is quite long, but the idea is very simple.  We want a model whose job is to guess the next character based on the previous $n$ letters. For example, having seen `ello`, the next characer is likely to be either a commma or space (if we assume is is the end of the word \"hello\"), or the letter `w` if we believe we are in the middle of the word \"mellow\". Humans are quite good at this, but of course seeing a larger history makes things easier (if we were to see 5 letters instead of 4, the choice between space and `w` would have been much easier).\n",
    "\n",
    "We will call $n$, the number of letters we need to guess based on, the _order_ of the language model.\n",
    "\n",
    "So, we are seeing $n$ letters, and need to guess the $n+1$th one. We are also given a large-ish amount of text (say, all of Shakespear works) that we can use. How would we go about solving this task?\n",
    "\n",
    "Mathematiacally, we would like to learn a function $P(c | h)$. Here, $c$ is a character, $h$ is a $n$-letters history, and $P(c|h)$ stands for how likely is it to see $c$ after we've seen $h$.\n",
    "\n",
    "Perhaps the simplest approach would be to just count and divide (a.k.a **maximum likelihood estimates**). We will count the number of times each letter $c'$ appeared after $h$, and divide by the total numbers of letters appearing after $h$. The **unsmoothed** part means that if we did not see a given letter following $h$, we will just give it a probability of zero.\n",
    "\n",
    "And that's all there is to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code\n",
    "Here is the code for training the model. `fname` is a file to read the characters from. `order` is the history size to consult (i.e., the $n$ in your $n$-gram). Note that we pad the data with leading `~` so that we also learn how to start (this is your `<START>` symbol in most language model notation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "\n",
    "def train_char_lm(fname, order=4):\n",
    "    with open(fname) as f:\n",
    "        data = '\\n'.join(f.readlines())\n",
    "    lm = defaultdict(Counter)\n",
    "    pad = \"~\" * order\n",
    "    data = pad + data\n",
    "    for i in range(len(data)-order):\n",
    "        history, char = data[i:i+order], data[i+order]\n",
    "        lm[history][char]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c,cnt/s) for c,cnt in counter.items()]\n",
    "\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./ROCStories__spring2016 - ROCStories_spring2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['storyid', 'storytitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].to_csv('./ROCstories2016.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it on some Shakespeare, which you can find on Canvas for SI 630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = train_char_lm(\"shakespeare_input.txt\", order=4)\n",
    "lm = train_char_lm('ROCstories2016.txt', order=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now let's do some queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('y', 0.16666666666666666),\n",
       " ('o', 0.08536585365853659),\n",
       " ('e', 0.2845528455284553),\n",
       " ('s', 0.22357723577235772),\n",
       " (' ', 0.17073170731707318),\n",
       " ('.', 0.052845528455284556),\n",
       " (',', 0.008130081300813009),\n",
       " ('f', 0.0040650406504065045),\n",
       " ('i', 0.0040650406504065045)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm['hell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 1.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm['Firs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 0.15833333333333333),\n",
       " ('s', 0.08666666666666667),\n",
       " ('e', 0.011666666666666667),\n",
       " ('t', 0.18766666666666668),\n",
       " ('p', 0.08133333333333333),\n",
       " ('b', 0.042333333333333334),\n",
       " ('i', 0.028),\n",
       " ('h', 0.04566666666666667),\n",
       " ('c', 0.062),\n",
       " ('g', 0.03933333333333333),\n",
       " ('f', 0.026333333333333334),\n",
       " ('A', 0.003),\n",
       " ('y', 0.008),\n",
       " ('m', 0.021),\n",
       " ('2', 0.0016666666666666668),\n",
       " ('v', 0.005),\n",
       " ('j', 0.01633333333333333),\n",
       " ('n', 0.017),\n",
       " ('I', 0.01),\n",
       " ('r', 0.021),\n",
       " ('a', 0.026333333333333334),\n",
       " ('o', 0.02666666666666667),\n",
       " ('J', 0.002),\n",
       " ('1', 0.0006666666666666666),\n",
       " ('k', 0.006333333333333333),\n",
       " ('l', 0.012),\n",
       " ('B', 0.003),\n",
       " ('w', 0.025),\n",
       " ('C', 0.004666666666666667),\n",
       " ('M', 0.0016666666666666668),\n",
       " ('R', 0.001),\n",
       " ('q', 0.002),\n",
       " ('S', 0.0026666666666666666),\n",
       " ('6', 0.0003333333333333333),\n",
       " ('4', 0.0006666666666666666),\n",
       " ('K', 0.0006666666666666666),\n",
       " ('3', 0.001),\n",
       " ('9', 0.0006666666666666666),\n",
       " ('u', 0.0013333333333333333),\n",
       " ('L', 0.0016666666666666668),\n",
       " ('P', 0.0006666666666666666),\n",
       " ('Y', 0.0006666666666666666),\n",
       " ('H', 0.0006666666666666666),\n",
       " ('5', 0.0003333333333333333),\n",
       " ('N', 0.0006666666666666666),\n",
       " ('D', 0.0006666666666666666),\n",
       " ('E', 0.0016666666666666668),\n",
       " ('T', 0.001),\n",
       " ('O', 0.0006666666666666666),\n",
       " ('\"', 0.0003333333333333333)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm['rst ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `ello` is primarily followed by either space, punctuation or `w` (or `r`, `u`, `n`), `Firs` is pretty much deterministic, and the word following `ist ` can start with pretty much every letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating from the model\n",
    "Generating is also very simple. To generate a letter, we will take the history, look at the last $order$ characteters, and then sample a random letter based on the corresponding distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def generate_letter(lm, history, order):\n",
    "        history = history[-order:]\n",
    "        dist = lm[history]\n",
    "        x = random()\n",
    "        for c,v in dist:\n",
    "            x = x - v\n",
    "            if x <= 0: return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a passage of $k$ characters, we just seed it with the initial history and run letter generation in a loop, updating the history at each turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, order, nletters=500):\n",
    "    history = \"~\" * order\n",
    "    out = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Shakespeare from different order models\n",
    "\n",
    "Let's try to generate text based on different language-model orders. Let's start with something silly:\n",
    "\n",
    "### order 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = train_char_lm(\"ROCstories2016.txt\", order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daving the caus vina pas the beca. Now toptingre eme. Sooll, alled clet anto at to preamin print. I use ind was a pulas eves in to alwas sady he cryondman sa's ing it intick holdentelf try. He dechilly. Fing the wat himet piz wayed, hand aborrentir was lowennef massy a quid alk went the lito she st a was do beighboucce grout and th any gavelfribre mad nigned a the suld shout a drad atchourprojew out but gothary waskiniverget hiso hare zook out by ass a but bett dause wask a schhin She whe decol.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so great.. but what if we increase the order to 4?\n",
    "\n",
    "### order 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = train_char_lm(\"ROCstories2016.txt\", order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan's who work one wered her job incomfortunately. Christmas the he woman mad. He went a uninja. Kat was dog computer dropped up.\n",
      "\n",
      "\"Amy had evening else.\n",
      "\n",
      "A littled out with her set. She were play walk and feet. He has bear mouth. He went ther to birthday. He ran into the affortunately was a labcoat a pet his a tracked than had way the stressing\n",
      "\n",
      "Lisa. I ask. After school and bulldog snowblow on.\n",
      "\n",
      "\"Brent to walked our Years of and have gave heard the trained close heard told because tooth. She d\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan's old his back.\n",
      "\n",
      "\"Tiffany keys acted what in 1987 to the figures, and she was able blue house. But some soon her per boss to review at to he grow animals. After was all disasterday.\n",
      "\n",
      "\"My moving on the mother typing. Not to keep on the slid could the kids wife and really decided up my fire that an and went with made store him to see the the rations. Kelly friend Megan the felt practers severy his deals. She punch. The saw the began to buy his knew friend the were to money. She decided about o\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already quite reasonable, and reads like English. Just 4 letters history! What if we increase it to 7?\n",
    "\n",
    "### order 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = train_char_lm(\"ROCstories2016.txt\", order=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan's phone. The new intern shrieked and kicked the picture wife did not own every day they are going to lunch and it was missing food. Suddenly, a small hole in a wheat flourish. After a year. Faith taking the costume to show. All of a store. Kacie decided to take the city street to rehearsal. She memory.\"\n",
      "\n",
      "Milly woke up, she needed a new video games all over Gina cried classified and pressed.\n",
      "\n",
      "\"Leo really bought he was done, I looked for an incredibly landed 4 feet and inviting his house. Sinc\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = train_char_lm(\"ROCstories2016.txt\", order=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan's parents called Lena's mom. Lena's mom found a giant, black, beast of a truck to the casino. At the commute. Tim began to break free. Hours later explained that Jesus loved everything went smoothly. Then, she called her bird.\n",
      "\n",
      "\"Nola lost her judgmental again.\"\n",
      "\n",
      "\"Jeff insulted her. The next day.\n",
      "\n",
      "\"Sam bought a chipmunk, but it was boring. Luckily, it ended up walking my dog. My dog got home and relax. He put out swan status to scare me.\"\n",
      "\n",
      "\"Stone was always sweet to the garbage men stopped by\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dfbce65314495b823d00c345429e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c71420858d64aeba83752ed7a2438b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f79c6b7791b4524b741561e371ea9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dce705076e34c48a7ae1bde267a3e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e5710df32c43b09604e889f9f66d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82c05755f614b4f847d8aa569311369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.9999998211860657, 1.0000003576278687],\n",
       " 'recall': [0.9999998211860657, 1.0000003576278687],\n",
       " 'f1': [0.9999998211860657, 1.0000003576278687],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.38.2)'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "# results = \n",
    "bertscore.compute(predictions=predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [generate_text(lm, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan\\'s parents took off his shoes. That\\'s when Sam decided he was just a little late, so had to go last week. I received the album. Then, Jo, turned blue. There were raccoons flew past Rick\\'s face. Rick grabbed him. He asked the pretty girl.\"\\n\\n\"Bill was extremely tired and fell. Jane ended up getting a flat iron. When I came back on Tom saw that the clients I\\'ve dealt with his dad. At first he was getting his new house, he started snowing. In my rush to get out. They told him he needed the car. I']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_first_n_lines(file_path, n=10000):\n",
    "    \"\"\"\n",
    "    Reads the first n lines from a file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the file.\n",
    "    - n: Number of lines to read.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the first n lines of the file.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for _ in range(n):\n",
    "                line = file.readline()\n",
    "                if not line:\n",
    "                    break  # Stop if the file ends before reaching n lines\n",
    "                lines.append(line.strip())  # Use strip() to remove newline characters\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "        return []\n",
    "\n",
    "    return lines\n",
    "\n",
    "# Since we can't execute file operations here, this is just an illustration.\n",
    "# You would replace `file_path` with the actual path to your 'ROCstories2016.txt' file and run this in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (this won't actually run here, it's just an example)\n",
    "file_path = './ROCstories2016.txt'\n",
    "first_10000_lines = read_first_n_lines(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = \"\\n\".join(first_10000_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2370445"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(referenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of predictions (1) and references (10000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bertscore\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mpredictions, references\u001b[39m=\u001b[39;49mfirst_10000_lines, lang\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/evaluate/module.py:450\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> 450\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/evaluate/module.py:541\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    536\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions and/or references don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match the expected format.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected format: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format\u001b[39m \u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput predictions: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput references: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(references)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 541\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of predictions (1) and references (10000)"
     ]
    }
   ],
   "source": [
    "bertscore.compute(predictions=predictions, references=first_10000_lines, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
