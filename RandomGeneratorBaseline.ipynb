{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Generator Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "np.random.seed(630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges the 2016 and 2017 ROCStories data\n",
    "df_2016 = pd.read_csv('./ROCStories__spring2016 - ROCStories_spring2016.csv')\n",
    "df_2017 = pd.read_csv('ROCStories_winter2017 - ROCStories_winter2017.csv')\n",
    "df = pd.concat([df_2016, df_2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a database of 5th sentences for the model to randomly select\n",
    "fifth_sentence_database = df['sentence5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the predictions and references for each instance in the data\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for index, instance in df.iterrows():\n",
    "    references.append(instance['sentence5'])\n",
    "    predictions.append(np.random.choice(fifth_sentence_database))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the BERT score for the random selection model\n",
    "bertscore = load(\"bertscore\")\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average BERT score results using the random selection model\n",
    "(np.mean(results['precision']), np.mean(results['recall']), np.mean(results['f1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average BERT score results:\n",
    "\n",
    "precision: 0.8686750045754013\n",
    "\n",
    "recall: 0.8686812542627632\n",
    "\n",
    "f1: 0.868567749891451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the METEOR score for the random selection model\n",
    "meteor = load('meteor')\n",
    "results = meteor.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average METEOR score results using the random selection model\n",
    "(np.mean(results['meteor']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average METEOR score results:\n",
    "\n",
    "meteor: 0.08249881396988645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the BLEU score for the random selection model\n",
    "bleu = load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average BLEU score results using the random selection model\n",
    "(np.mean(results['bleu']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average BLEU score results:\n",
    "\n",
    "bleu: 0.0010766664237931724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the ROUGE score for the random selection model\n",
    "rouge = load('rouge')\n",
    "results = rouge.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average ROUGE score results using the random selection model\n",
    "(np.mean(results['rogue1']), np.mean(results['rogue2']), np.mean(results['rougeL']), np.mean(results['rougeLsum']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average ROGUE score results:\n",
    "\n",
    "rogue1: 0.07026043691351706\n",
    "\n",
    "rogue2: 0.0021983239810122322\n",
    "\n",
    "rogueL: 0.06483072944486867\n",
    "\n",
    "rogueLsum: 0.06482816798651193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the Perplexity score for the predictions from the random selection model\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(predictions=predictions, model_id='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average ePerplexity score results using the predictions from the random selection model\n",
    "results['mean_perplexity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average perplexity results:\n",
    "\n",
    "perplexity: 140.05386473811177"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
