{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-3.5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in ROCStories data\n",
    "df = pd.read_csv('../data/ROCStories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompt for each row \n",
    "prompts = []\n",
    "for _, row in df.iterrows():\n",
    "    story = row['sentence1'] + ' ' + row['sentence2'] + ' ' + row['sentence3'] + ' ' + row['sentence4']\n",
    "    prompt = \"Write a concluding sentence for the following story: \\'\" + story\n",
    "    prompts.append(prompt)\n",
    "\n",
    "df['prompt'] = pd.Series(prompts)\n",
    "\n",
    "df.to_csv('ROCStoriesPrompt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"https://openai.vocareum.com/v1\", api_key=\"voc-547088396116581323079566219891017280.17915057\")\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    prompt = row['prompt']\n",
    "    completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    prediction = completion.choices[0].message.content\n",
    "    df['generated_conclusion'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the BERT score\n",
    "bertscore = load(\"bertscore\")\n",
    "BERT_results = bertscore.compute(predictions=list(df['generated_conclusion']), references=df['sentence5'], lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average BERT score results\n",
    "(np.mean(BERT_results['precision']), np.mean(BERT_results['recall']), np.mean(BERT_results['f1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average BERT score results:\n",
    "\n",
    "precision: 0.8690111303031445\n",
    "\n",
    "recall: 0.8882322731614113\n",
    "\n",
    "f1: 0.87843736743927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the METEOR score for the random selection model\n",
    "meteor = load('meteor')\n",
    "results = meteor.compute(predictions=list(df['generated_conclusion']), references=df['sentence5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average METEOR score results using the random selection model\n",
    "(np.mean(results['meteor']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average METEOR score results:\n",
    "\n",
    "meteor: 0.2061267535387586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the BLEU score for the random selection model\n",
    "bleu = load(\"bleu\")\n",
    "results = bleu.compute(predictions=list(df['generated_conclusion']), references=df['sentence5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average BLEU score results\n",
    "(np.mean(results['bleu']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average BLEU score results:\n",
    "\n",
    "bleu: 0.013144386609249357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the ROUGE score\n",
    "rouge = load('rouge')\n",
    "results = rouge.compute(predictions=list(df['generated_conclusion']), references=df['sentence5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average ROUGE score\n",
    "(np.mean(results['rouge1']), np.mean(results['rouge2']), np.mean(results['rougeL']), np.mean(results['rougeLsum']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average ROUGE score results:\n",
    "\n",
    "rouge1: 0.1860120896453826\n",
    "\n",
    "rouge2: 0.030390927104791972\n",
    "\n",
    "rougeL: 0.15273339894584584\n",
    "\n",
    "rougeLsum: 0.15275016545396286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the Perplexity score\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(predictions=list(df['generated_conclusion']), references=df['sentence5'], model_id='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average Perplexity score\n",
    "results['mean_perplexity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Perplexity results:\n",
    "\n",
    "Perplexity: 48.88060186958313"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
